@article{scikit-image,
 title = {scikit-image: image processing in {P}ython},
 author = {van der Walt, {S}t\'efan and {S}ch\"onberger, {J}ohannes {L}. and
           {Nunez-Iglesias}, {J}uan and {B}oulogne, {F}ran\c{c}ois and {W}arner,
           {J}oshua {D}. and {Y}ager, {N}eil and {G}ouillart, {E}mmanuelle and
           {Y}u, {T}ony and the scikit-image contributors},
 year = {2014},
 month = {6},
 keywords = {Image processing, Reproducible research, Education,
             Visualization, Open source, Python, Scientific programming},
 volume = {2},
 pages = {e453},
 journal = {PeerJ},
 issn = {2167-8359},
 url = {https://doi.org/10.7717/peerj.453},
 doi = {10.7717/peerj.453}
}


@InProceedings{networkx,
  author =       {Aric A. Hagberg and Daniel A. Schult and Pieter J. Swart},
  title =        {Exploring Network Structure, Dynamics, and Function using NetworkX},
  booktitle =   {Proceedings of the 7th Python in Science Conference},
  pages =     {11 - 15},
  address = {Pasadena, CA USA},
  year =      {2008},
  editor =    {Ga\"el Varoquaux and Travis Vaught and Jarrod Millman},
}


@misc{makesense,
  title = {Make Sense (Alpha Version)},
  author =       {Piotr Skalski},
  howpublished = {\url{https://www.makesense.ai/}},
  note = {Accessed: 20-05-2020}
}

@article{foody_effect_1995,
	title = {The effect of training set size and composition on artificial neural network classification},
	volume = {16},
	issn = {0143-1161, 1366-5901},
	url = {https://www.tandfonline.com/doi/full/10.1080/01431169508954507},
	doi = {10.1080/01431169508954507},
	language = {en},
	number = {9},
	urldate = {2020-05-02},
	journal = {International Journal of Remote Sensing},
	author = {Foody, G.M. and McCULLOCH, M. B. and Yates, W. B.},
	month = jun,
	year = {1995},
	pages = {1707--1723},
	file = {Foody et al. - 1995 - The effect of training set size and composition on.pdf:/Users/timogi/Zotero/storage/BUHRABY3/Foody et al. - 1995 - The effect of training set size and composition on.pdf:application/pdf}
}

@article{hestness_deep_2017,
	title = {Deep {Learning} {Scaling} is {Predictable}, {Empirically}},
	url = {http://arxiv.org/abs/1712.00409},
	abstract = {Deep learning (DL) creates impactful advances following a virtuous recipe: model architecture search, creating large training data sets, and scaling computation. It is widely believed that growing training sets and models should improve accuracy and result in better products. As DL application domains grow, we would like a deeper understanding of the relationships between training set size, computational scale, and model accuracy improvements to advance the state-of-the-art. This paper presents a large scale empirical characterization of generalization error and model size growth as training sets grow. We introduce a methodology for this measurement and test four machine learning domains: machine translation, language modeling, image processing, and speech recognition. Our empirical results show power-law generalization error scaling across a breadth of factors, resulting in power-law exponents---the "steepness" of the learning curve---yet to be explained by theoretical work. Further, model improvements only shift the error but do not appear to affect the power-law exponent. We also show that model size scales sublinearly with data size. These scaling relationships have significant implications on deep learning research, practice, and systems. They can assist model debugging, setting accuracy targets, and decisions about data set growth. They can also guide computing system design and underscore the importance of continued computational scaling.},
	urldate = {2020-05-02},
	journal = {arXiv:1712.00409 [cs, stat]},
	author = {Hestness, Joel and Narang, Sharan and Ardalani, Newsha and Diamos, Gregory and Jun, Heewoo and Kianinejad, Hassan and Patwary, Md Mostofa Ali and Yang, Yang and Zhou, Yanqi},
	month = dec,
	year = {2017},
	note = {arXiv: 1712.00409},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/timogi/Zotero/storage/RALV7XIM/Hestness et al. - 2017 - Deep Learning Scaling is Predictable, Empirically.pdf:application/pdf;arXiv.org Snapshot:/Users/timogi/Zotero/storage/9YFKAZXC/1712.html:text/html}
}
