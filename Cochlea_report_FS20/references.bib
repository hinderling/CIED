@article{scikit-image,
 title = {scikit-image: image processing in {P}ython},
 author = {van der Walt, {S}t\'efan and {S}ch\"onberger, {J}ohannes {L}. and
           {Nunez-Iglesias}, {J}uan and {B}oulogne, {F}ran\c{c}ois and {W}arner,
           {J}oshua {D}. and {Y}ager, {N}eil and {G}ouillart, {E}mmanuelle and
           {Y}u, {T}ony and the scikit-image contributors},
 year = {2014},
 month = {6},
 keywords = {Image processing, Reproducible research, Education,
             Visualization, Open source, Python, Scientific programming},
 volume = {2},
 pages = {e453},
 journal = {PeerJ},
 issn = {2167-8359},
 url = {https://doi.org/10.7717/peerj.453},
 doi = {10.7717/peerj.453}
}


@InProceedings{networkx,
  author =       {Aric A. Hagberg and Daniel A. Schult and Pieter J. Swart},
  title =        {Exploring Network Structure, Dynamics, and Function using NetworkX},
  booktitle =   {Proceedings of the 7th Python in Science Conference},
  pages =     {11 - 15},
  address = {Pasadena, CA USA},
  year =      {2008},
  editor =    {Ga\"el Varoquaux and Travis Vaught and Jarrod Millman},
}


@misc{makesense,
  title = {Make Sense (Alpha Version)},
  author =       {Piotr Skalski},
  howpublished = {\url{https://www.makesense.ai/}},
  note = {Accessed: 20-05-2020}
}

@article{foody_effect_1995,
	title = {The effect of training set size and composition on artificial neural network classification},
	volume = {16},
	issn = {0143-1161, 1366-5901},
	url = {https://www.tandfonline.com/doi/full/10.1080/01431169508954507},
	doi = {10.1080/01431169508954507},
	language = {en},
	number = {9},
	urldate = {2020-05-02},
	journal = {International Journal of Remote Sensing},
	author = {Foody, G.M. and McCULLOCH, M. B. and Yates, W. B.},
	month = jun,
	year = {1995},
	pages = {1707--1723},
	file = {Foody et al. - 1995 - The effect of training set size and composition on.pdf:/Users/timogi/Zotero/storage/BUHRABY3/Foody et al. - 1995 - The effect of training set size and composition on.pdf:application/pdf}
}

@article{hestness_deep_2017,
	title = {Deep {Learning} {Scaling} is {Predictable}, {Empirically}},
	url = {http://arxiv.org/abs/1712.00409},
	abstract = {Deep learning (DL) creates impactful advances following a virtuous recipe: model architecture search, creating large training data sets, and scaling computation. It is widely believed that growing training sets and models should improve accuracy and result in better products. As DL application domains grow, we would like a deeper understanding of the relationships between training set size, computational scale, and model accuracy improvements to advance the state-of-the-art. This paper presents a large scale empirical characterization of generalization error and model size growth as training sets grow. We introduce a methodology for this measurement and test four machine learning domains: machine translation, language modeling, image processing, and speech recognition. Our empirical results show power-law generalization error scaling across a breadth of factors, resulting in power-law exponents---the "steepness" of the learning curve---yet to be explained by theoretical work. Further, model improvements only shift the error but do not appear to affect the power-law exponent. We also show that model size scales sublinearly with data size. These scaling relationships have significant implications on deep learning research, practice, and systems. They can assist model debugging, setting accuracy targets, and decisions about data set growth. They can also guide computing system design and underscore the importance of continued computational scaling.},
	urldate = {2020-05-02},
	journal = {arXiv:1712.00409 [cs, stat]},
	author = {Hestness, Joel and Narang, Sharan and Ardalani, Newsha and Diamos, Gregory and Jun, Heewoo and Kianinejad, Hassan and Patwary, Md Mostofa Ali and Yang, Yang and Zhou, Yanqi},
	month = dec,
	year = {2017},
	note = {arXiv: 1712.00409},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/timogi/Zotero/storage/RALV7XIM/Hestness et al. - 2017 - Deep Learning Scaling is Predictable, Empirically.pdf:application/pdf;arXiv.org Snapshot:/Users/timogi/Zotero/storage/9YFKAZXC/1712.html:text/html}
}


@inproceedings{ronneberger_u-net_2015,
	address = {Cham},
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	isbn = {978-3-319-24574-4},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} – {MICCAI} 2015},
	publisher = {Springer International Publishing},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
	year = {2015},
	pages = {234--241},
	file = {Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf:/Users/timogi/Zotero/storage/5M2YHSAB/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf:application/pdf}
}


@article{wu_towards_2015,
	title = {Towards dropout training for convolutional neural networks},
	volume = {71},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608015001446},
	doi = {10.1016/j.neunet.2015.07.007},
	abstract = {Recently, dropout has seen increasing use in deep learning. For deep convolutional neural networks, dropout is known to work well in fully-connected layers. However, its effect in convolutional and pooling layers is still not clear. This paper demonstrates that max-pooling dropout is equivalent to randomly picking activation based on a multinomial distribution at training time. In light of this insight, we advocate employing our proposed probabilistic weighted pooling, instead of commonly used max-pooling, to act as model averaging at test time. Empirical evidence validates the superiority of probabilistic weighted pooling. We also empirically show that the effect of convolutional dropout is not trivial, despite the dramatically reduced possibility of over-fitting due to the convolutional architecture. Elaborately designing dropout training simultaneously in max-pooling and fully-connected layers, we achieve stateof-the-art performance on MNIST, and very competitive results on CIFAR-10 and CIFAR-100, relative to other approaches without data augmentation. Finally, we compare max-pooling dropout and stochastic pooling, both of which introduce stochasticity based on multinomial distributions at pooling stage.},
	language = {en},
	urldate = {2020-03-10},
	journal = {Neural Networks},
	author = {Wu, Haibing and Gu, Xiaodong},
	month = nov,
	year = {2015},
	pages = {1--10},
	file = {Wu und Gu - 2015 - Towards dropout training for convolutional neural .pdf:/Users/timogi/Zotero/storage/MJRV8DVC/Wu und Gu - 2015 - Towards dropout training for convolutional neural .pdf:application/pdf}
}



@article{litjens_survey_2017,
	title = {A {Survey} on {Deep} {Learning} in {Medical} {Image} {Analysis}},
	volume = {42},
	issn = {13618415},
	url = {http://arxiv.org/abs/1702.05747},
	doi = {10.1016/j.media.2017.07.005},
	abstract = {Deep learning algorithms, in particular convolutional networks, have rapidly become a methodology of choice for analyzing medical images. This paper reviews the major deep learning concepts pertinent to medical image analysis and summarizes over 300 contributions to the field, most of which appeared in the last year. We survey the use of deep learning for image classification, object detection, segmentation, registration, and other tasks and provide concise overviews of studies per application area. Open challenges and directions for future research are discussed.},
	urldate = {2020-03-06},
	journal = {Medical Image Analysis},
	author = {Litjens, Geert and Kooi, Thijs and Bejnordi, Babak Ehteshami and Setio, Arnaud Arindra Adiyoso and Ciompi, Francesco and Ghafoorian, Mohsen and van der Laak, Jeroen A. W. M. and van Ginneken, Bram and Sánchez, Clara I.},
	month = dec,
	year = {2017},
	note = {arXiv: 1702.05747},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {60--88},
	file = {arXiv Fulltext PDF:/Users/timogi/Zotero/storage/QQGGRT2S/Litjens et al. - 2017 - A Survey on Deep Learning in Medical Image Analysi.pdf:application/pdf;arXiv.org Snapshot:/Users/timogi/Zotero/storage/YVW989UZ/1702.html:text/html}
}

